# -*- coding: utf-8 -*-
"""PRML_Minor_Project(B21EE055 & B21EE043).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pGsVxARTLGzYGR6vlgcxDnEp3OcTIzQd
"""

import warnings
warnings.filterwarnings("ignore");

"""DataPrep is used in the code to create detailed report of the dataset."""

!pip install dataprep

"""## Importing import libraries and the dataset."""

import pandas as pd
import numpy as np
import math 
import plotly.express as px
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn import svm
from scipy.linalg import svd
from sklearn.cluster import DBSCAN
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import plotly.tools as tls
from sklearn.model_selection import KFold, cross_val_score
from matplotlib import pyplot
import seaborn
from numpy.linalg import eig
from numpy.linalg import inv
import matplotlib.pyplot as pyplt
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from sklearn.metrics import classification_report
import os
from dataprep.eda import create_report

"""here we are importing the dataset from google drive """

from google.colab import drive
drive.mount('/content/drive')

retail_dataset = pd.read_csv("/content/drive/MyDrive/content2/Online-Retail-_1_.csv.csv.xls")
retail_dataset

"""#Preprocessing of the dataset

Separation of data into time, date, month and year column to make dataset more informative.
"""

val=len(retail_dataset['InvoiceDate']);
col1=[];
col2=[];
col3=[];
col4=[];
## traversing the invoice date column and separating the data time month and year by traversing the string 
for i in range(0,val):
  strr=retail_dataset['InvoiceDate'][i];
  date="";
  i=0;

#3 handaling the month 
  while(strr[i]!='/'):
    date=date+strr[i];
    i=i+1;
  month="";
  i=i+1;

## handling the date. 
  while(strr[i]!='/'):
    month=month+strr[i];
    i=i+1;
  year="";
  i=i+1;

## handling the time 
  while(strr[i]!=' '):
    year=year+strr[i];
    i=i+1;  
  time="";
  i=i+1;

## storing the time 
  while(i<len(strr)):
    time=time+strr[i];
    i=i+1;

## converting all values to int 
  yearr=int(year);
  month=int(month);
  date=int(date);

## appending all in one column 
  col1.append(date);
  col2.append(month);
  col3.append(year);
  col4.append(time);

## taking the time as average like 7:30 to 8:30 tending to 8;
col5=[];

for k in col4:

  vall=k;
  ll=len(vall);
  i=0;
  ti="";

  while(vall[i]!=':'):
    ti=ti+vall[i];
    i=i+1;
  i=i+1;

  if vall[i]=='0' or vall[i]=='2' or vall[i]=='1':
    tii=int(ti);
    col5.append(tii);
  else:
    tii=int(ti);
    col5.append(tii+1);

## converting into dataframe 
col1=pd.DataFrame(col1);
col2=pd.DataFrame(col2);
col3=pd.DataFrame(col3);
col5=pd.DataFrame(col5);

## adding to the whole dataset 
retail_dataset=retail_dataset.drop('InvoiceDate',axis=1)
retail_dataset['Date']=col2;
retail_dataset['Month']=col1;
retail_dataset['Year']=col3;
retail_dataset['Time']=col5;
print(retail_dataset);

retail_dataset['Time'] = pd.cut(x=retail_dataset['Time'], bins=[6, 9, 12, 15, 19, 21])


from sklearn import preprocessing  ## i encoded some columns in my dataset using labelencoder . 
label_encoder = preprocessing.LabelEncoder()
retail_dataset['Time']=  label_encoder.fit_transform(retail_dataset['Time'])  


print("Maximum value of time is",retail_dataset['Time'].max())
print("Minimum value of time is",retail_dataset['Time'].min())
print("Minimum value of Date is",retail_dataset['Date'].min())
print("Maximum value of Date is",retail_dataset['Date'].max())
print("Minimum value of Month is",retail_dataset['Month'].min())
print("Maximum value of Month is",retail_dataset['Month'].max())
print("Maximum value of Year is",retail_dataset['Year'].max())
print("Minimum value of Year is",retail_dataset['Year'].min())

"""Here now we will check for unique values and the null values present in each column of the dataset. This is done here to detect the noisy data and outliers in the dataset so that they can be treated accordingly."""

retail_dataset.nunique()

print("NULL values in the dataset:\n",retail_dataset.isnull().sum());

"""Here in the belows cells we are deleting the rows with null values in column Description."""

retail_dataset=retail_dataset.dropna(subset=['Description']);
print("NULL values in the dataset after deletion of rows of Description with NULL values:\n",retail_dataset.isnull().sum());

"""We got an idea that total price may be more useful here for this dataset. So we have created a new column for total price below."""

totalprice=((retail_dataset['Quantity']).T)*(retail_dataset['UnitPrice']);
retail_dataset['TotalPrice']=totalprice;

# retail_dataset=retail_dataset.drop(['Quantity','UnitPrice'],axis=1);

"""#Handling Outliers

We checked the presence of outliers in some specific columns chosen based on the type of column (column with some sort of distribution). Box plots may be useful here but due to the large amount of data we are unable to go with box plots. 
The presence of an outlier is decided if the data in these columns is out of range (μ-3σ,μ+3σ). The index of such rows are stored and then these rows are deleted from the dataset.
"""

#handling outliers
delete_arr=[];
for cols in ['Quantity','UnitPrice','TotalPrice']:
  mean_val=retail_dataset[cols].mean();
  std_val=retail_dataset[cols].std();
  min_val=int(mean_val-2*std_val);
  max_val=int(mean_val+2*std_val);
  col_vals=np.array(retail_dataset[cols]);
  for i in range(0,10000):
    # if(type(col_vals[i])==int):
      if(int(col_vals[i])<min_val or int(col_vals[i])>max_val):
        delete_arr.append(i);
    # else:
    #   delete_arr.append(i);

retail_dataset.drop(retail_dataset.index[delete_arr], axis=0, inplace=True);
  # retail_dataset=retail_dataset.drop(index=[delete_arr], axis=0, inplace=True);

print(retail_dataset.shape)

"""Description column contains some ambiguous data. From our observation  we find that most of the noisy data in this column doesn’t have a capital letter beginning whereas the useful data is Starting with a capital letter. So we removed these rows from our dataset. 

"""

#for column discription analysis
rows,cols=retail_dataset.shape;
index_arr2=[];
for i in range(0,rows):
  char_i=retail_dataset.iloc[i,2];
  ord_st=ord(char_i[0]);
  if(ord_st>90):
    index_arr2.append(i);
  elif(ord_st<65):
    index_arr2.append(i);

# dropping the rows from Description which contains non useful values
retail_dataset.drop(retail_dataset.index[index_arr2], axis=0,inplace=True);
retail_dataset.shape

"""Our dataset must contain only non-negative for a selected number of columns(unit price, quantity) . So we checked for negative values in those columns and if such data is present it is removed from the dataset.

"""

# checking the existence of negative numbers in quantity column and removing those rows
inp_cols=[3,4];
for ind in inp_cols:
  index_quant=[];
  rows,cols=retail_dataset.shape;
  for i in range(0,rows):
    if(int(retail_dataset.iloc[i,ind])<0):
      index_quant.append(i);
  retail_dataset.drop(retail_dataset.index[index_quant], axis=0, inplace=True);

retail_dataset

"""#Encoding selected columns"""

#Encoding the columns which are either strings or are needed to be encoded
column_names=['StockCode','Description','Country','InvoiceNo'];
for col in column_names:
  le=LabelEncoder();

  values = le.fit_transform(retail_dataset[col]);
  retail_dataset[col]=values[:];
retail_dataset

"""#Handling NULL values in CustomerID column

We came up with an idea of filling the null values based on a classification algorithm trained on rows with non-null values. The null values of each row is then filled with the prediction by this classification algorithm.
"""

dataframe_notnull=retail_dataset[retail_dataset['CustomerID'].notnull()];
dataframe_notnullx=dataframe_notnull.drop(['CustomerID'],axis=1);
dataframe_notnully=dataframe_notnull['CustomerID'];
#encoding this above dataframe y
le=LabelEncoder();
values_le = le.fit_transform(dataframe_notnully);
dataframe_notnully=values_le[:];
dataframe_null=retail_dataset[retail_dataset['CustomerID'].isnull()];
dataframe_nullx=dataframe_null.drop(['CustomerID'],axis=1);
print(dataframe_null)
rows_null,cols_null=dataframe_null.shape;

#filling the null values in custumerID column using DT classifier
dt_classifier=DecisionTreeClassifier();
dt_classifier.fit(dataframe_notnullx[:][:300000],dataframe_notnully[:][:300000]);
values_dt=dt_classifier.predict(dataframe_nullx);
dataframe_null['CustomerID']=values_dt;

retail_dataset = pd.concat([dataframe_notnull,dataframe_null]);

#count of remaining null values after taking kare of null values in the two columns
print("NULL values in the dataset after processing CostumerID column:\n",retail_dataset.isnull().sum());

#Droping invoiceNo and Year as they are useful here for this dataset
retail_dataset=retail_dataset.drop(['InvoiceNo','Year'],axis=1);

"""#Final Dataset"""

retail_dataset

"""#Visualization """

netlist_month=[];
netcost_month=[];
for j in range(1,13):
  d1=retail_dataset[retail_dataset['Month']==j];
  ls=np.unique(d1['CustomerID'])
  netcost_month.append(len(ls));
  netsell=0;  
  h1=d1['Quantity'];
  h1=np.array(h1);
  h2=d1['UnitPrice'];
  h2=np.array(h2);
  for i in range(0,len(h1)):    
    netsell=netsell + (h1[i]*h2[i])
  netlist_month.append(netsell);

netlist_date=[];
netcost_date=[];
for j in range(1,32):
  d1=retail_dataset[retail_dataset['Date']==j];
  ls=np.unique(d1['CustomerID'])
  netcost_date.append(len(ls));
  netsell=0;  
  h1=d1['Quantity'];
  h1=np.array(h1);
  h2=d1['UnitPrice'];
  h2=np.array(h2);
  for i in range(0,len(h1)):    
    netsell=netsell + (h1[i]*h2[i])
  netlist_date.append(netsell);

netlist_time=[];
netcost_time=[];

for j in range(0,6):
  d1=retail_dataset[retail_dataset['Time']==j];
  ls=np.unique(d1['CustomerID'])
  netcost_time.append(len(ls));
  netsell=0;  
  h1=d1['Quantity'];
  h1=np.array(h1);
  h2=d1['UnitPrice'];
  h2=np.array(h2);
  for i in range(0,len(h1)):    
    netsell=netsell + (h1[i]*h2[i])
  netlist_time.append(netsell);

plt.bar([0,1,2,3,4,5],netlist_time)
plt.xlabel("TIME")
plt.ylabel("Total sell")
plt.show();
plt.bar([0,1,2,3,4,5],netcost_time)
plt.xlabel("TIME")
plt.ylabel("Total Costumers visited")
plt.show();

## for the date;
li=[];
for i in range(1,32):
  li.append(i);
plt.bar(li,netlist_date)
plt.xlabel("Date")
plt.ylabel("Total sell")
plt.show();
plt.bar(li,netcost_date)
plt.xlabel("Date")
plt.ylabel("Total Costumers visited")
plt.show();

## for the month
li2=[];
for i in range(1,13):
  li2.append(i);
plt.bar(li2,netlist_month);
plt.xlabel("Month")
plt.ylabel("Total sell")
plt.show();
plt.bar(li2,netcost_month);
plt.xlabel("Month")
plt.ylabel("Total Costumers visited")
plt.show()

#plotting pie chart to visualize the total expenses in each country
country_expenses=[];
x_val=[];
for i in range(0,37):
  d1=retail_dataset[retail_dataset['Country']==i];
  sum=d1['TotalPrice'].sum();
  country_expenses.append(sum);
  x_val.append(i);
df_country=pd.DataFrame(data=np.array([x_val,country_expenses]).T,columns=['x1','country_expenses'])
df_country.plot.pie(y='country_expenses', figsize=(5, 5))

#pie chart to visualize the expenses on monthly basis
month_expenses=[];
x2_val=[];
for i in range(1,13):
  d1=retail_dataset[retail_dataset['Month']==i];
  sum=d1['TotalPrice'].sum();
  month_expenses.append(sum);
  x2_val.append(i);
df_month=pd.DataFrame(data=np.array([x2_val,month_expenses]).T,columns=['x1','month_expenses'])
df_month.plot.pie(y='month_expenses', figsize=(5, 5))

for col in retail_dataset.columns:
  retail_un=retail_dataset[col].value_counts();
  print("Count of individual values of column",col,"\n",retail_un);
  print(" ");

for col in ['StockCode','Description','Country']:
  seaborn.displot(retail_dataset, x=col, binwidth=5);
  print(" ");
  pyplt.show();

report_retail = create_report(retail_dataset)
report_retail.show()

retail_dataset=retail_dataset.drop(['TotalPrice'],axis=1);

"""# PCA (Principle Component Analysis
Here we applied PCA and then using the eigenenergy computed the best no. of n_ component.
"""

## importing the important libraries. 

from sklearn.decomposition import PCA
from numpy.linalg import eig
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import MinMaxScaler

retail_dataset1=retail_dataset.copy();
retail_dataset1=pd.DataFrame(retail_dataset1)

## now apply pca to convert data in continuous dataset using pca so that kmeans can be applied. 

def reduced_data1(data,k): 
  data1=data.copy();
  modpca = PCA(n_components = k) 
  modpca=modpca.fit(data1)
  newdata=modpca.transform(data1);
  ll=np.cumsum(modpca.explained_variance_ratio_);
  print(ll);
  return ll,newdata;

## this will only return the reduced dataset 

def reduced_data(data,k): 
  data1=data.copy();
  modpca = PCA(n_components = k) 
  modpca=modpca.fit(data1)
  newdata=modpca.transform(data1);
  ll=modpca.explained_variance_ratio_;
  return newdata;

"""Plot of Eigen-energy is shown below."""

## now transfor the data into continuous data using pca. 

list_of_eign,retail_dataset2=reduced_data1(retail_dataset1,retail_dataset1.shape[1])
retail_dataset2=pd.DataFrame(retail_dataset2)


## now plot the eignvalues with the features.
 
nn=[];
for i in range(0,retail_dataset1.shape[1]):
  nn.append(i+1);

plt.plot(nn,list_of_eign)
plt.xlabel("No. of features")
plt.ylabel("commutative sum of Eigon value")
plt.grid();
plt.show();

"""# Kmeans Clustering (Elbow method)
Here we applied the elbow method to find the best no. of clusters for this trained the k means model and found parameters on that model and plotted 
that and the point where I am getting more steepness that is my best no. of clusters. 

"""

# function that will apply kmeans clustering to the data. and will give us the best results.
 
def find_best_clusters(data):
  listof_cluster=[];
  for i in range(2,10):
    listof_cluster.append(i);
  store_out=[]; ## in this list appending the values 
  score_mat=[];
  for i in listof_cluster:
    model=KMeans(n_clusters=i); ## now traing and fit the modle 
    model=model.fit(data);
    moo=model.predict(data);
    acci=model.inertia_;   ## predict the inerial value. 
    store_out.append(acci);# print(slis);  
    # print(store_out)
  plt.plot(listof_cluster,store_out);
  plt.grid(); ## plot it in a grid 
  plt.show();

find_best_clusters(reduced_data(retail_dataset2,5))

"""##Kmeans and elbow method on original dataset (without PCA)
Apply Kmeans on the original dataset then the elbow method to find the best cluster
"""

find_best_clusters(retail_dataset)

"""##Function to create random sample of dataset
Here we have created a function that will give us the random generated subset of dataset dataset. 
"""

import random
def create(k,data):
  newl=[]; 
  # print(data.shape[0]) ## in this list we will store all the rows that should be merged to make a dataframe;
  for i in range(0,k):  ## now in this function we will generate 1000 random numbers and will pic that rows from actual data frame and append in a list. 
    val=random.randint(0,data.shape[0]-1); # generate random number
    ne2=data.iloc[[val]]; ## extract row 
    newl.append(ne2); 
  datan3=pd.concat(newl);
  datan3=pd.DataFrame(datan3); 
  return datan3;

"""##Final K-means
Now training the k means model on pca applied data and using n_clusters=3(best value of cluster by elbow method).
"""

## as we got the best no. of clusters now we will train our final model and will get the desired results. 
model_trained=KMeans(n_clusters=3); ## now traing and fit the modle 
## doing the feature reduction 
newreduces=reduced_data(retail_dataset2,3)
newreduces=pd.DataFrame(newreduces);
## training the model 
model_trained=model_trained.fit(newreduces);
rand_data=create(100000,newreduces);
moo=model_trained.predict(rand_data); 
## storing the cluster centroids  
listt = model_trained.cluster_centers_   
listt=pd.DataFrame(listt);

"""## Performance of the model
 Calculating valrious scores for the clusters formed for the dataset using kmeans.
"""

## calculating the scores.

the_score=acci=model_trained.inertia_;  
the_silhoutee_score =silhouette_score(rand_data, moo) 

print("The value of SSE score for K-means on PCA reduced dataset is ",the_score);
print("The Silhoutee Score for K-means on PCA reduced dataset is ",the_silhoutee_score);

from sklearn.metrics import davies_bouldin_score
from sklearn.metrics import calinski_harabasz_score

## computing other type of scores. 

s = calinski_harabasz_score(rand_data,moo)
DB = davies_bouldin_score(rand_data,moo)
print("The value of calinski_harabasz score for K-means on PCA reduced dataset is ",s);
print("The value of davies_bouldin score for K-means on PCA reduced dataset is ",DB)

"""##Data Visualization
Plotting the 2d scatter plot(by taking the features with highest eigon values.)
"""

plt.scatter(rand_data.iloc[:,0],rand_data.iloc[:,1],c=moo)
plt.scatter(listt.iloc[:,0],listt.iloc[:,1],color='red');## plotting the centroids of clusters. 
plt.show();

"""Plotting the 3d scatter plot along with centroids."""

import plotly.graph_objects;

fig = plotly.graph_objects.Figure(data=[
        plotly.graph_objects.Scatter3d(
            x=rand_data.iloc[:,0], y=rand_data.iloc[:,1], z=rand_data.iloc[:,2],
            mode='markers',
            marker=dict(size=5, color=moo, opacity=1)
        ),
        plotly.graph_objects.Scatter3d(
            x=listt.iloc[:, 0], y=listt.iloc[:, 1], z=listt.iloc[:, 2], mode='markers',
            marker=dict(size=40, color='red', opacity=1)
        )
    ])
fig.show();

"""##Final K-means on without PCA dataset (original dataset)
Training the kmeans model on Without pca data with same no. of clusters. 
"""

model_trained1=KMeans(n_clusters=3); ## now traing and fit the modle 
model_trained1=model_trained1.fit(retail_dataset2);

## creating the random data and predicting output. 
rand_data1=create(100000,retail_dataset2);
moo3=model_trained1.predict(rand_data1); 

## storing the centroids 
listt1 = model_trained1.cluster_centers_   
listt1=pd.DataFrame(listt1);

## computing the score values. 
the_score1=acci=model_trained1.inertia_;  
print("the value of sse without pca ",the_score1);
the_silhoutee_score1 =silhouette_score(rand_data1, moo3)    
print("the silhoutee score is without ",the_silhoutee_score1);

## computing other score values. 
from sklearn.metrics import calinski_harabasz_score
s1 = calinski_harabasz_score(rand_data1,moo3)
from sklearn.metrics import davies_bouldin_score
DB1 = davies_bouldin_score(rand_data1,moo3)

print(s1);
print(DB1)

"""##Plots
 plotting the 2d scatter plot along with centroids.(taken two features with highest eigen values.
"""

plt.scatter(rand_data1.iloc[:,0],rand_data1.iloc[:,1],c=moo3)
plt.scatter(listt1.iloc[:,0],listt1.iloc[:,1],color='red'); ## plotting the centroids of clusters. 
plt.show();

"""Plotting the 3d scatter plot. """

import plotly.graph_objects
fig = plotly.graph_objects.Figure(data=[
        plotly.graph_objects.Scatter3d(
            x=rand_data1.iloc[:,0], y=rand_data1.iloc[:,1], z=rand_data1.iloc[:,2],
            mode='markers',
            marker=dict(size=5, color=moo3, opacity=1)
        ),
        plotly.graph_objects.Scatter3d(
            x=listt1.iloc[:, 0], y=listt1.iloc[:, 1], z=listt1.iloc[:, 2], mode='markers',
            marker=dict(size=10, color='red', opacity=1)
        )
    ])
fig.show();

"""# Heirarichal Clustering on PCA dataset
Sampling is done because the computation cost is too high for whole dataset. 
"""

from sklearn.cluster import AgglomerativeClustering; ## training our model 
modeltr=AgglomerativeClustering(n_clusters=3,linkage='single') ## training the model 

rand_data=create(50000,newreduces);
moo2=modeltr.fit_predict(rand_data);

"""##Performance
Computing the score values for this clustering results.
"""

the_silhoutee_score2 =silhouette_score(rand_data, moo2)    
print("The silhoutee score is ",the_silhoutee_score2);
s2 = calinski_harabasz_score(rand_data1,moo3)
from sklearn.metrics import davies_bouldin_score
DB2 = davies_bouldin_score(rand_data1,moo3)
print("The davies_boulding_score is",DB2);
print("The calinski_harabasz_score is ",s2)

"""##2D scatter plot. """

plt.scatter(rand_data.iloc[:,0],rand_data.iloc[:,1],c=moo2)
plt.show();

"""##3D scatter plot """

## the 3d scatter plot 
import plotly.express as plpx
plott = plpx.scatter_3d(rand_data.iloc[:,0:3], x=0, y=1, z=2,color=moo2)
plott.show()

"""#DBSCAN Algorithm
Sampling to compensiate computation cost. Calculating max distance for a cluster point.
"""

max_norm=0;
norm_arr=[];
for i in range(0,100):
  a1=np.array(pd.DataFrame(reduced_data(retail_dataset2,3)).iloc[i,:]);
  b1=np.array(pd.DataFrame(reduced_data(retail_dataset2,3)).iloc[i+2000,:]);
  norm_store=np.linalg.norm(a1-b1);
  norm_arr.append(norm_store);
  if(norm_store>max_norm):
    max_norm=norm_store;
print(max_norm)
print(norm_arr)

"""Creating the smaller dataset by sampling"""

def create(k,data):
  newl=[];  ## in this list we will store all the rows that should be merged to make a dataframe;
  for i in range(0,k):  ## now in this function we will generate 1000 random numbers and will pic that rows from actual data frame and append in a list. 
    val=np.random.randint(0,data.shape[0]); # generate random number
    ne2=data.iloc[[val]]; ## extract row 
    newl.append(ne2); 
  datan3=pd.concat(newl);
  datan3=pd.DataFrame(datan3);
  return datan3;

## creating the small dataset 

small_dataset=create(20000,pd.DataFrame(reduced_data(retail_dataset2,3))); ## creating the datset 
clustering_DBSCAN = DBSCAN(eps=3000, min_samples=100).fit(small_dataset);

"""##3D Scatter Plot"""

## taking out the labels generated by the model 
labels=clustering_DBSCAN.labels_ 

## plotting the 3d plot 
import plotly.express as plpx
plott = plpx.scatter_3d(small_dataset.iloc[:,0:], x=0, y=1, z=2,color=labels);
plott.show();

"""## Performance of the model
Computing the score values. (silhoutee_score, calinski_harabasz_score,davies_bouldin_score). 
"""

the_silhoutee_score2 =silhouette_score(rand_data, moo2)    
print("the silhoutee score is ",the_silhoutee_score2);

from sklearn.metrics import calinski_harabasz_score
s = calinski_harabasz_score(small_dataset, labels)
from sklearn.metrics import davies_bouldin_score
DB = davies_bouldin_score(small_dataset, labels)
print("The davies_boulding_score is",DB);
print("The calinski_harabasz_score is ",s)